# 퍼시스턴트 볼륨(PV)과 퍼시스턴트 볼륨 클레임(PVC)
- 데이터베이스처럼 포드 내부에서 특정 데이터를 보유해야 하는 Stateful 애플리케이션은 데이터를 어떻게 관리할지 고민해야 한다.
  - 디플로이먼트를 삭제하면 포드도 삭제되고, 동시에 포드의 데이터도 삭제되기 때문이다.
- 쿠버네티스에서는 어느 노드에서도 접근하여 사용할 수 있는 **퍼시스턴트 볼륨**을 사용하여 해결한다.
  - 퍼시스턴트 볼륨은 워커 노드들이 네트워크상에서 스토리지를 마운트해 영속적으로 데이터를 저장할 수 있는 볼륨이다.
  - 따라서, 포드에 장애가 생겨 다른 노드로 옮기더라도 해당 노드에서 퍼시스턴트 볼륨에 네트워크로 연결하여 데이터를 계속 사용할 수 있다.

## 1. 로컬 볼륨 : hostPath, emptyDir
- **hostPath**는 호스트와 볼륨을 공유하기 위해 사용하는 볼륨이다.
- **emptyDir**은 포드의 컨테이너 간에 볼륨을 공유하기 위해서 사용하는 볼륨이다.

### 1.1. 워커 노드의 로컬 디렉토리를 볼륨으로 사용 : hostPath
- 포드의 데이터를 보존할 수 있는 가장 간단한 방법은 호스트의 디렉토리를 포드와 고융하여 데이터를 저장하는 것이다.
- 호스트와 디렉토리를 공유하는 포드를 생성하기 위한 YAML 파일을 작성한다.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-pod
spec:
  containers:
    - name: my-container
      image: busybox
      args: [ "tail", "-f", "/dev/null" ]
      volumeMounts:
      - name: my-hostpath-volume
        mountPath: /etc/data
  volumes:
    - name: my-hostpath-volume
      hostPath:
        path: /tmp
```
- volumes 항목에 볼륨을 정의한 뒤, 이를 정의하는 containers 항목에서 참조하여 사용한다.
  - 볼륨에서 hostPath 항목을 정의함으로써 호스트의 /tmp를 포드의 /etc/data에 마운트하였다.
  - 포드를 생성한 뒤 포드의 컨테이너가 내부로 들어가 /etc/data 디렉토리에 파일을 생성하면 호스트의 /tmp 디렉토리에 파일이 저장된다.
  - 포드 컨테이너의 /etc/data와 호스트의 /tmp는 동일한 디렉토리로써 사용되는 것이다.
- 디플로이먼트의 포드에 장애가 생겨 다른 노드로 포드가 옮겨갔을 경우, 이전 노드에 저장된 데이터를 사용할 수 없기 때문에, 이러한 방식의 데이터 보존은 바람직하지 않다.
- hostpath 볼륨은 모든 노드에 배치해야 하는 특수한 포드의 경우에 유용하게 사용할 수 있다.

### 1.2. 포드 내의 컨테이너 간 임시 데이터 공유 : emptyDir
- emptyDir 볼륨은 포드가 실행되는 도중에만 필요한 휘발성 데이터를 각 컨테이너가 함께 사용할 수 있도록 임시 저장 공간을 생성한다.
- emptyDir 디렉토리는 비어있는 상태로 생성되며 포드가 삭제되면 emptyDir에 저장되어 있던 데이터도 함께 삭제된다.
- emptyDir은 한 컨테이너가 파일을 관리하고 한 컨테이너가 그 파일을 사용하는 경우에 유용하게 사용할 수 있다.

## 2. 네트워크 볼륨
- 쿠버네티스에서는 별도의 플러그인을 설치하지 않아도 다양한 종류의 네트워크 볼륨을 포드에 마운트할 수 있다.
  - 클라우드 플랫폼의 볼륨을 포드에 마운트할 수도 있다.
- 네트워크 볼륨의 위치는 네트워크로 접근할 수만 있으면 어느 곳에 존재해도 상관없다.
  - 단, AWS의 EBS와 같은 클라우드에 종속적인 볼륨을 사용하려면, AWS에서 쿠버네티스 클러스터를 생성할 때 특정 클라우드를 위한 옵션이 별도로 설정되어 있어야 한다.
  - kops를 통해 AWS에서 설치했다면 자동으로 이 옵션이 설정된다. (--cloud-provider)

### 2.1. NFS를 네트워크 볼륨으로 사용하기
- **NFS**(Network File System)는 대부분의 운영체제에서 사용할 수 있는 네트워크 스토리지로, 여러 개의 클라이언트가 동시에 마운트하여 사용할 수 있다.
  - 여러 개의 스토리지를 클러스터링하는 다른 솔루션에 비해 안정성이 떨어지나, 하나의 서버만으로 간편하게 사용할 수 있으며, NFS를 로컬 스토리지처럼 사용할 수 있다.
- NFS를 사용하려면 NFS 서버와 NFS 클라이언트가 필요하다.
  - NFS 서버는 영속적인 데이터가 실제로 저장되는 네트워크 스토리지 서버이다.
  - NFS 클라이언트는 NFS 서버에 마운트하여 스토리지에 파일을 읽고 쓰는 역할을 한다.
    - NFS 클라이언트는 워커 노드의 기능을 사용하므로, NFS 서버만 별도로 구축한다.
- nfs-deployment.yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-server
spec:
  selector:
    matchLabels:
      role: nfs-server
  template:
    metadata:
      labels:
        role: nfs-server
    spec:
      containers:
      - name: nfs-server
        image: gcr.io/google_containers/volume-nfs:0.8
        ports:
          - name: nfs
            containerPort: 2049
          - name: mountd
            containerPort: 20048
          - name: rpcbind
            containerPort: 111
        securityContext:
          privileged: true
```
- nfs-service.yaml
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nfs-service
spec:
  ports:
  - name: nfs
    port: 2049
  - name: mountd
    port: 20048
  - name: rpcbind
    port: 111
  selector:
    role: nfs-server
```
- NFS 서버를 위한 디플로이먼트와 서비스를 생성하고, 해당 NFS 서버의 볼륨을 포드에서 마운트하여 데이터를 영속적으로 저장한다.
- nfs-pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nfs-pod
spec:
  containers:
    - name: nfs-mount-container
      image: busybox
      args: [ "tail", "-f", "/dev/null" ]
      volumeMounts:
      - name: nfs-volume
        mountPath: /mnt
  volumes:
  - name : nfs-volume
    nfs:
      path: /
      server: {NFS_SERVICE_IP}
```
- mountPath를 /mnt로 설정했기 때문에 NFS 서버의 네트워크 볼륨은 포드 컨테이너의 /mnt 디렉토리에 마운트된다.
  - 즉, 컨테이너 내부에서 /mnt 디렉토리에 파일을 저장하면 실제로는 NFS 서버에 데이터가 저장된다.
  - 또한, volume 항목에서 nfs라는 항목을 정의함으로써 NFS 서버의 볼륨을 사용한다고 명시한다.
- NFS 볼륨의 마운트는 컨테이너 내부가 아닌 워커 노드에서 발생하므로, 서비스의 DNS 이름으로 NFS 서버에 접근할 수 없다.
  - 노드에서는 포드의 IP로 통신할 수 있지만, 쿠버네티스의 DNS를 사용하도록 설정되어 있지 않기 때문이다.
  - 따라서, NFS 서비스의 Cluster IP를 직접 얻은 뒤, YAML 파일에 사용하는 방식으로 포드를 생성한다.